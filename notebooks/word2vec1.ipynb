{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b41c006-4306-4814-a8c2-5f9bb72a63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a077eb9-e355-4ef1-9137-9d51fd263682",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a116a95-b4db-46fe-a58e-6b573a1581d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/lemmatized_sentences.csv\")\n",
    "df2 = pd.read_csv(\"../data/stemmed_sentences.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5836d294-cfb1-4cc5-a58b-2354dd8dec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = [\"0\"]\n",
    "\n",
    "# NaN değerleri ve boş stringleri temizle\n",
    "df1 = df1.dropna()\n",
    "df1 = df1[df1[\"0\"].str.strip() != \"\"]\n",
    "\n",
    "df2.columns = [\"0\"]\n",
    "\n",
    "# NaN değerleri ve boş stringleri temizle\n",
    "df2 = df2.dropna()\n",
    "df2 = df2[df2[\"0\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ca9a8c-3383-4f92-b5ab-c0dc578bca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doğru tokenizasyon fonksiyonu\n",
    "def proper_tokenize(text):\n",
    "    # Özel karakterleri kaldır ve küçük harfe çevir\n",
    "    text = re.sub(r'[^a-zA-ZğüşıöçĞÜŞİÖÇ\\s]', '', text.lower())\n",
    "    # NLTK ile tokenize et\n",
    "    tokens = word_tokenize(text)\n",
    "    # Stopwords'leri ve tek karakterli kelimeleri kaldır\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab688d90-548f-4d38-bc56-99eabfa8cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doğru tokenizasyon uygula\n",
    "df1['tokens'] = df1['0'].apply(proper_tokenize)\n",
    "df2['tokens'] = df2['0'].apply(proper_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ecdecc-b454-4121-b0df-7cba2ae1cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token listelerini oluştur\n",
    "tokenized_corpus_lemmatized = df1['tokens'].tolist()\n",
    "tokenized_corpus_stemmed = df2['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab5ae91-43c8-4121-af8e-facc162e915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model(corpus, param, model_prefix):\n",
    "    model_type = param['model_type']\n",
    "    vector_size = param['vector_size']\n",
    "    window = param['window']\n",
    "    \n",
    "    # CBOW (sg=0) veya Skip-gram (sg=1)\n",
    "    sg = 0 if model_type == 'cbow' else 1\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=1,\n",
    "        workers=4,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    model_filename = f\"{model_prefix}_{model_type}_vs{vector_size}_w{window}.model\"\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd3caa7d-ca45-43ca-b967-4ef1cb0ce11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ../models/lemmatized_model_cbow_vs100_w2.model\n",
      "Model saved as ../models/lemmatized_model_skipgram_vs100_w2.model\n",
      "Model saved as ../models/lemmatized_model_cbow_vs100_w4.model\n",
      "Model saved as ../models/lemmatized_model_skipgram_vs100_w4.model\n",
      "Model saved as ../models/lemmatized_model_cbow_vs300_w2.model\n",
      "Model saved as ../models/lemmatized_model_skipgram_vs300_w2.model\n",
      "Model saved as ../models/lemmatized_model_cbow_vs300_w4.model\n",
      "Model saved as ../models/lemmatized_model_skipgram_vs300_w4.model\n",
      "Model saved as ../models/stemmed_model_cbow_vs100_w2.model\n",
      "Model saved as ../models/stemmed_model_skipgram_vs100_w2.model\n",
      "Model saved as ../models/stemmed_model_cbow_vs100_w4.model\n",
      "Model saved as ../models/stemmed_model_skipgram_vs100_w4.model\n",
      "Model saved as ../models/stemmed_model_cbow_vs300_w2.model\n",
      "Model saved as ../models/stemmed_model_skipgram_vs300_w2.model\n",
      "Model saved as ../models/stemmed_model_cbow_vs300_w4.model\n",
      "Model saved as ../models/stemmed_model_skipgram_vs300_w4.model\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"../models/lemmatized_model\")\n",
    "\n",
    "# Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"../models/stemmed_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13ff9e8-2a5f-47cb-9a8a-e340f0323128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dosyalarını yüklemek\n",
    "model_1 = Word2Vec.load(\"../models/lemmatized_model_cbow_vs100_w2.model\")\n",
    "model_2 = Word2Vec.load(\"../models/lemmatized_model_cbow_vs100_w4.model\")\n",
    "model_3 = Word2Vec.load(\"../models/lemmatized_model_cbow_vs300_w2.model\")\n",
    "model_4 = Word2Vec.load(\"../models/lemmatized_model_cbow_vs300_w4.model\")\n",
    "model_5 = Word2Vec.load(\"../models/lemmatized_model_skipgram_vs100_w2.model\")\n",
    "model_6 = Word2Vec.load(\"../models/lemmatized_model_skipgram_vs100_w4.model\")\n",
    "model_7 = Word2Vec.load(\"../models/lemmatized_model_skipgram_vs300_w2.model\")\n",
    "model_8 = Word2Vec.load(\"../models/lemmatized_model_skipgram_vs300_w4.model\")\n",
    "model_9  = Word2Vec.load(\"../models/stemmed_model_cbow_vs100_w2.model\")\n",
    "model_10 = Word2Vec.load(\"../models/stemmed_model_cbow_vs100_w4.model\")\n",
    "model_11 = Word2Vec.load(\"../models/stemmed_model_cbow_vs300_w2.model\")\n",
    "model_12 = Word2Vec.load(\"../models/stemmed_model_cbow_vs300_w4.model\")\n",
    "model_13 = Word2Vec.load(\"../models/stemmed_model_skipgram_vs100_w2.model\")\n",
    "model_14 = Word2Vec.load(\"../models/stemmed_model_skipgram_vs100_w4.model\")\n",
    "model_15 = Word2Vec.load(\"../models/stemmed_model_skipgram_vs300_w2.model\")\n",
    "model_16 = Word2Vec.load(\"../models/stemmed_model_skipgram_vs300_w4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52e78fa-93e2-4686-9744-ec1576fb4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'angara' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırmak\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity = model.wv.most_similar(\"angara\", topn=3)\n",
    "    print(f\"\\n{model_name} Modeli - 'angara' ile En Benzer 3 Kelime:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06b834f8-87d8-46c0-baba-48123337eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized CBOW Window 2 Dim 100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9783456921577454\n",
      "Kelime: pc, Benzerlik Skoru: 0.9757558107376099\n",
      "Kelime: grilled, Benzerlik Skoru: 0.9722403883934021\n",
      "\n",
      "Stemmed Skipgram Window 4 Dim 100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9821490049362183\n",
      "Kelime: fry, Benzerlik Skoru: 0.9809262752532959\n",
      "Kelime: grilled, Benzerlik Skoru: 0.9781044125556946\n",
      "\n",
      "Lemmatized Skipgram Window 2 Dim 300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9948694705963135\n",
      "Kelime: grilled, Benzerlik Skoru: 0.9799842834472656\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9784950017929077\n",
      "\n",
      "lemmatized skipgram window 4 dim 100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9957451224327087\n",
      "Kelime: fried, Benzerlik Skoru: 0.9880256056785583\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9879208207130432\n",
      "\n",
      "lemmatized cbow window 2 dim 300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9279224276542664\n",
      "Kelime: peri, Benzerlik Skoru: 0.8516044020652771\n",
      "Kelime: chicken, Benzerlik Skoru: 0.8489097952842712\n",
      "\n",
      "lemmatizedskipgramwindow 2 dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.8686208128929138\n",
      "Kelime: chicken, Benzerlik Skoru: 0.8470597863197327\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.7715723514556885\n",
      "\n",
      "lemmatized_cbow_window 4_dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9447627067565918\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9101870059967041\n",
      "Kelime: grilled, Benzerlik Skoru: 0.8525151610374451\n",
      "\n",
      "lemmatized_skipgram_window4_dim300.model Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9183423519134521\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9030472636222839\n",
      "Kelime: peri, Benzerlik Skoru: 0.8309979438781738\n",
      "\n",
      "stemmed_cbow_window2_dim100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9931115508079529\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.982586681842804\n",
      "Kelime: salt, Benzerlik Skoru: 0.9814422726631165\n",
      "\n",
      "stemmed_skipgram_window2_dim100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.994360089302063\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.990071713924408\n",
      "Kelime: grill, Benzerlik Skoru: 0.9807732701301575\n",
      "\n",
      "stemmed_cbow_window4_dim100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9973545670509338\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9896073937416077\n",
      "Kelime: sauc, Benzerlik Skoru: 0.9880678653717041\n",
      "\n",
      "stemmed_skipgram_window4_dim100 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9963263869285583\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9946663975715637\n",
      "Kelime: fri, Benzerlik Skoru: 0.991743266582489\n",
      "\n",
      "stemmed_cbow_window2_dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9246216416358948\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9010791778564453\n",
      "Kelime: peri, Benzerlik Skoru: 0.8693962097167969\n",
      "\n",
      "stemmed_skipgram_window2_dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9136983156204224\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.8881509900093079\n",
      "Kelime: grill, Benzerlik Skoru: 0.8577719926834106\n",
      "\n",
      "stemmed_cbow_window4_dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9349559545516968\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9276250004768372\n",
      "Kelime: peri, Benzerlik Skoru: 0.902198851108551\n",
      "\n",
      "stemmed_skipgram_window4_dim300 Modeli - 'angara' ile En Benzer 3 Kelime:\n",
      "Kelime: jamaican, Benzerlik Skoru: 0.9152730107307434\n",
      "Kelime: tangdi, Benzerlik Skoru: 0.9073976278305054\n",
      "Kelime: grill, Benzerlik Skoru: 0.8614106178283691\n"
     ]
    }
   ],
   "source": [
    "# 16 model için benzer kelimeleri yazdır\n",
    "print_similar_words(model_1, \"Lemmatized CBOW Window 2 Dim 100\")\n",
    "print_similar_words(model_2, \"Stemmed Skipgram Window 4 Dim 100\")\n",
    "print_similar_words(model_3, \"Lemmatized Skipgram Window 2 Dim 300\")\n",
    "print_similar_words(model_4, \"lemmatized skipgram window 4 dim 100\")\n",
    "print_similar_words(model_5, \"lemmatized cbow window 2 dim 300\")\n",
    "print_similar_words(model_6, \"lemmatizedskipgramwindow 2 dim300\")\n",
    "print_similar_words(model_7, \"lemmatized_cbow_window 4_dim300\")\n",
    "print_similar_words(model_8, \"lemmatized_skipgram_window4_dim300.model\")\n",
    "print_similar_words(model_9, \"stemmed_cbow_window2_dim100\")\n",
    "print_similar_words(model_10, \"stemmed_skipgram_window2_dim100\")\n",
    "print_similar_words(model_11, \"stemmed_cbow_window4_dim100\")\n",
    "print_similar_words(model_12, \"stemmed_skipgram_window4_dim100\")\n",
    "print_similar_words(model_13, \"stemmed_cbow_window2_dim300\")\n",
    "print_similar_words(model_14, \"stemmed_skipgram_window2_dim300\")\n",
    "print_similar_words(model_15, \"stemmed_cbow_window4_dim300\")\n",
    "print_similar_words(model_16, \"stemmed_skipgram_window4_dim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f2fb9a-f47b-45b2-b6aa-979ee838d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En sık kullanılan 20 kelime: [('chicken', 9241), ('pizza', 8531), ('peri', 6018), ('grilled', 5230), ('garlic', 2711), ('bone', 2707), ('bread', 2525), ('tender', 2513), ('jamaican', 2505), ('paneer', 2491), ('melt', 2373), ('fry', 2083), ('bageecha', 2051), ('cheese', 1934), ('pide', 1872), ('murgh', 1731), ('amritsari', 1731), ('fried', 1728), ('seekh', 1714), ('angara', 1473)]\n"
     ]
    }
   ],
   "source": [
    "# Veri setinizde en sık geçen 20 kelime\n",
    "from collections import Counter\n",
    "all_words = [word for sentence in tokenized_corpus_lemmatized for word in sentence]\n",
    "print(\"En sık kullanılan 20 kelime:\", Counter(all_words).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f2357-a380-40cb-96e8-4cfbcf767bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
